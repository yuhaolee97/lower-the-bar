{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03cb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Embedding,Bidirectional,GRU,LSTM,Dense,Dropout,SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12758078",
   "metadata": {},
   "source": [
    "Read in `train.csv` which contains 2 columns:\n",
    "1. df['Text']: Each row can be a huge chunk of text. The code below will split them into shorter sequences\n",
    "2. df['Category']: One of prediction categories, e.g. Family, Criminal, etc\n",
    "\n",
    "This `train.csv` can be really flexible, so you can plug any dataset you want into it. Due to time constraints, the dataset we used for the actual app was not large enough and not well-curated enough. However, after deployment it seemed to do okay, but definitely had edge cases and adversarial examples could easily be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cfc1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae666db9",
   "metadata": {},
   "source": [
    "Text processing:\n",
    "1. Remove non-alphabets\n",
    "2. Make lowercase. Case-sensitive won't work well because in actual deployment, laymen probably don't care about typing with correct case\n",
    "3. Split by spaces into a list\n",
    "4. Stem words. Rationale: the dataset we used was small. But we need to be able to train on \"divorce/divorces/divorced\" and still predict based on \"divorcee\"\n",
    "5. Join back the text into a big string. The tokenizing will occur later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ad8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "X_processed = []\n",
    "for text in df['Text']:\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    X_processed.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e821a90",
   "metadata": {},
   "source": [
    "Fit, then use, the Keras Tokenizer on entire text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_processed)\n",
    "X_sequences = tokenizer.texts_to_sequences(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_index_len = len(tokenizer.word_index)\n",
    "print(tokenizer_index_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938378a4",
   "metadata": {},
   "source": [
    "Split large sequences with 1000+ tokens into 20-length sequences\n",
    "- Previously: ~150 samples, each a huge chunk of text\n",
    "- After the next step: ~10,000 samples, each a fixed 20-length sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7875901",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 20\n",
    "X = []\n",
    "y_corpus = []\n",
    "\n",
    "for i in range(len(X_sequences)):\n",
    "    num_sequences = len(X_sequences[i]) // sequence_len\n",
    "    \n",
    "    for j in range(num_sequences):\n",
    "        start = sequence_len * j\n",
    "        end = sequence_len * (j + 1)\n",
    "        X.append(X_sequences[i][start:end])\n",
    "        y_corpus.append(df['Category'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c823f35",
   "metadata": {},
   "source": [
    "One-hot encode the Categories (e.g. Family, Criminal) because there is no 'ordering' of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f9944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)\n",
    "\n",
    "Y = pd.get_dummies(pd.Series(y_corpus))\n",
    "Y = Y[ sorted(Y.columns) ]\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(Y.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce4f89",
   "metadata": {},
   "source": [
    "Perform train_test_split with a small 10% test set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1281544",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca6d2a",
   "metadata": {},
   "source": [
    "Create Deep Learning Model\n",
    "- Train Embedding layer (but we really should have used GloVe pre-trained so it can generalize better in actual deployment)\n",
    "- Bi-directional GRU model (bi-directional to capture context from both sides, GRU to balance training time and performance between RNN and LSTM\n",
    "- Some SpatialDropout, dropout, recurrent_dropout for regularization because we need to use this DL model for actual deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe957623",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = tokenizer_index_len + 1\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "# model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66528e9b",
   "metadata": {},
   "source": [
    "Train model, with EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa450e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d37e0",
   "metadata": {},
   "source": [
    "Save both the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "model.save('saved_model')\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345c66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc86ac3",
   "metadata": {},
   "source": [
    "Code below is old and not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c082a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# X = pad_sequences(X_sequences, maxlen=sequence_len)\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f0f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_len = 20\n",
    "# X_corpus = []\n",
    "# y_corpus = []\n",
    "\n",
    "# for i in range(len(df['Text'])):\n",
    "#     text = df['Text'][i].split()\n",
    "#     num_sequences = len(text) // sequence_len\n",
    "    \n",
    "#     for j in range(num_sequences):\n",
    "#         start = sequence_len * j\n",
    "#         end = sequence_len * (j + 1)\n",
    "#         X_corpus.append(' '.join(text[start:end]))\n",
    "#         y_corpus.append(df['Category'][i])\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# max_words = 10000\n",
    "# tokenizer = Tokenizer(num_words=max_words)\n",
    "# tokenizer.fit_on_texts(X_corpus)\n",
    "# X_sequences = tokenizer.texts_to_sequences(X_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
